2025/02/28 09:46:35 - mmengine - INFO - Working dir: out/eval_local/occ3d/tpv/onlycam/version_1
2025/02/28 09:46:36 - mmengine - INFO - Config:
_dim_ = 256
_ffn_dim_ = 512
_num_cams_ = 6
_num_levels_ = 4
_pos_dim_ = 128
batch_size = 1
data_aug_conf = dict(
    H=900,
    W=1600,
    bot_pct_lim=(
        0.0,
        0.0,
    ),
    final_dim=(
        896,
        1600,
    ),
    rand_flip=False,
    resize_lim=(
        1.0,
        1.0,
    ),
    rot_lim=(
        0.0,
        0.0,
    ))
data_root = 'data/nuscenes/'
dataset_tag = 'occ3d'
find_unused_parameters = False
grad_max_norm = 35
grid_size = [
    200,
    200,
    16,
]
img_freq = 1000
img_norm_cfg = dict(
    mean=[
        103.53,
        116.28,
        123.675,
    ], std=[
        1.0,
        1.0,
        1.0,
    ], to_rgb=False)
inspect = False
load_from = 'ckpts/r101_dcn_fcos3d_pretrain.pth'
loss = dict(
    kl_loss=False,
    loss_cfgs=[
        dict(
            manual_class_weight=tensor([
                1.0155, 1.0690, 1.3001, 1.0725, 0.9464, 1.1009, 1.2696, 1.0626,
                1.1890, 1.0622, 1.0060, 0.8571, 1.0392, 0.9087, 0.8936, 0.8549,
                0.8528, 0.5000
            ]),
            multi_loss_weights=dict(
                loss_voxel_ce_weight=10.0, loss_voxel_lovasz_weight=1.0),
            num_classes=18,
            type='OccupancyLoss',
            weight=1.0),
        dict(mask_elems=None, type='RenderLoss', weight=None),
    ],
    mask_elems=False,
    occupancy_weight=1.0,
    render_weight=10.0,
    type='MultiLoss',
    w_occupancy=True)
loss_input_convertion = dict(pred_occ='pred_occ', render='render')
lr = 0.0002
max_epochs = 20
model = dict(
    aggregator=dict(
        hidden_dims=512,
        in_dims=256,
        nbr_classes=18,
        occ_shape=[
            200,
            200,
            16,
        ],
        out_dims=256,
        pre_render_kwargs=dict(
            dataset_tag='occ3d',
            mask_elements=False,
            overwrite_opacity=False,
            overwrite_rotations=False,
            overwrite_scales=True,
            transfer_colors=True,
            transfer_opacity=False,
            use_offsets=False),
        render=True,
        render_kwargs=dict(
            apply_sigmoid=False,
            cam_idx=None,
            dataset_tag='occ3d',
            entropy_filtering=False,
            gaussian_op=1.0,
            gaussian_scale=None,
            inspect=False,
            iso_bev_rendering=False,
            num_classes=18,
            pc_range=[
                -40,
                -40,
                -1,
                40,
                40,
                5.4,
            ],
            render_gt_mode=None,
            render_ncam=1,
            render_per_channel=False,
            voxel_size=0.4,
            with_bev_depth_rendering=True,
            with_bev_rendering=True,
            with_cam_rendering=True,
            with_depth_rendering=True),
        scale_h=2,
        scale_w=2,
        scale_z=2,
        tpv_h=100,
        tpv_w=100,
        tpv_z=8,
        type='TPVAggregator',
        voxel_size=0.4),
    head=dict(
        embed_dims=256,
        encoder=dict(
            num_layers=3,
            num_points_in_pillar=[
                4,
                32,
                32,
            ],
            pc_range=[
                -40,
                -40,
                -1,
                40,
                40,
                5.4,
            ],
            return_intermediate=False,
            tpv_h=100,
            tpv_w=100,
            tpv_z=8,
            transformerlayers=dict(
                attn_cfgs=[
                    dict(
                        embed_dims=256,
                        num_levels=1,
                        type='TPVCrossViewHybridAttention'),
                    dict(
                        deformable_attention=dict(
                            embed_dims=256,
                            floor_sampling_offset=False,
                            num_levels=4,
                            num_points=[
                                8,
                                64,
                                64,
                            ],
                            num_z_anchors=[
                                4,
                                32,
                                32,
                            ],
                            tpv_h=100,
                            tpv_w=100,
                            tpv_z=8,
                            type='TPVMSDeformableAttention3D'),
                        embed_dims=256,
                        tpv_h=100,
                        tpv_w=100,
                        tpv_z=8,
                        type='TPVImageCrossAttention'),
                ],
                feedforward_channels=512,
                ffn_dropout=0.1,
                operation_order=(
                    'self_attn',
                    'norm',
                    'cross_attn',
                    'norm',
                    'ffn',
                    'norm',
                ),
                type='TPVFormerLayer'),
            type='TPVFormerEncoder'),
        num_cams=6,
        num_feature_levels=4,
        positional_encoding=dict(
            col_num_embed=100,
            num_feats=128,
            row_num_embed=100,
            type='LearnedPositionalEncoding'),
        tpv_h=100,
        tpv_w=100,
        tpv_z=8,
        type='TPVFormerHead'),
    img_backbone=dict(
        dcn=dict(deform_groups=1, fallback_on_stride=False, type='DCNv2'),
        depth=101,
        frozen_stages=1,
        norm_cfg=dict(requires_grad=False, type='BN2d'),
        norm_eval=True,
        num_stages=4,
        out_indices=(
            1,
            2,
            3,
        ),
        stage_with_dcn=(
            False,
            False,
            True,
            True,
        ),
        style='caffe',
        type='ResNet'),
    img_backbone_out_indices=[
        1,
        2,
        3,
    ],
    img_neck=dict(
        add_extra_convs='on_output',
        in_channels=[
            512,
            1024,
            2048,
        ],
        num_outs=4,
        out_channels=256,
        relu_before_extra_convs=True,
        start_level=0,
        type='FPN'),
    pc_range=[
        -40,
        -40,
        -1,
        40,
        40,
        5.4,
    ],
    type='BEVSegmentor',
    use_grid_mask=True,
    voxel_size=0.4)
model_tag = 'tpvformer'
num_classes = 18
num_points = [
    8,
    64,
    64,
]
num_points_in_pillar = [
    4,
    32,
    32,
]
occ_path = './data/gts/'
optimizer = dict(
    optimizer=dict(lr=0.0002, type='AdamW', weight_decay=0.0001),
    paramwise_cfg=dict(custom_keys=dict(img_backbone=dict(lr_mult=0.1))))
out_occ_shapes = [
    [
        200,
        200,
        16,
    ],
]
point_cloud_range = [
    -40,
    -40,
    -1,
    40,
    40,
    5.4,
]
print_freq = 250
scale_h = 2
scale_w = 2
scale_z = 2
test_dataset_config = dict(
    data_aug_conf=dict(
        H=900,
        W=1600,
        bot_pct_lim=(
            0.0,
            0.0,
        ),
        final_dim=(
            896,
            1600,
        ),
        rand_flip=False,
        resize_lim=(
            1.0,
            1.0,
        ),
        rot_lim=(
            0.0,
            0.0,
        )),
    data_root='data/nuscenes/',
    imageset='data/bevdetv2-nuscenes_infos_val.pkl',
    is_mini=False,
    phase='val',
    pipeline=[
        dict(to_float32=True, type='LoadMultiViewImageFromFiles'),
        dict(occ_path='./data/gts/', type='LoadOccupancyOcc3dNuscenes'),
        dict(type='ResizeCropFlipImage'),
        dict(
            mean=[
                103.53,
                116.28,
                123.675,
            ],
            std=[
                1.0,
                1.0,
                1.0,
            ],
            to_rgb=False,
            type='NormalizeMultiviewImage'),
        dict(type='DefaultFormatBundle'),
        dict(num_cams=6, type='NuScenesAdaptor', use_ego=True),
    ],
    type='Occ3dNuScenesDataset')
test_loader = dict(batch_size=1, num_workers=8, shuffle=False)
test_pipeline = [
    dict(to_float32=True, type='LoadMultiViewImageFromFiles'),
    dict(occ_path='./data/gts/', type='LoadOccupancyOcc3dNuscenes'),
    dict(type='ResizeCropFlipImage'),
    dict(
        mean=[
            103.53,
            116.28,
            123.675,
        ],
        std=[
            1.0,
            1.0,
            1.0,
        ],
        to_rgb=False,
        type='NormalizeMultiviewImage'),
    dict(type='DefaultFormatBundle'),
    dict(num_cams=6, type='NuScenesAdaptor', use_ego=True),
]
tpv_h_ = 100
tpv_w_ = 100
tpv_z_ = 8
train_dataset_config = dict(
    data_aug_conf=dict(
        H=900,
        W=1600,
        bot_pct_lim=(
            0.0,
            0.0,
        ),
        final_dim=(
            896,
            1600,
        ),
        rand_flip=False,
        resize_lim=(
            1.0,
            1.0,
        ),
        rot_lim=(
            0.0,
            0.0,
        )),
    data_root='data/nuscenes/',
    imageset='data/bevdetv2-nuscenes_infos_train.pkl',
    is_mini=False,
    phase='train',
    pipeline=[
        dict(to_float32=True, type='LoadMultiViewImageFromFiles'),
        dict(occ_path='./data/gts/', type='LoadOccupancyOcc3dNuscenes'),
        dict(type='ResizeCropFlipImage'),
        dict(type='PhotoMetricDistortionMultiViewImage'),
        dict(
            mean=[
                103.53,
                116.28,
                123.675,
            ],
            std=[
                1.0,
                1.0,
                1.0,
            ],
            to_rgb=False,
            type='NormalizeMultiviewImage'),
        dict(type='DefaultFormatBundle'),
        dict(num_cams=6, type='NuScenesAdaptor', use_ego=True),
    ],
    type='Occ3dNuScenesDataset')
train_loader = dict(batch_size=1, num_workers=8, shuffle=True)
train_pipeline = [
    dict(to_float32=True, type='LoadMultiViewImageFromFiles'),
    dict(occ_path='./data/gts/', type='LoadOccupancyOcc3dNuscenes'),
    dict(type='ResizeCropFlipImage'),
    dict(type='PhotoMetricDistortionMultiViewImage'),
    dict(
        mean=[
            103.53,
            116.28,
            123.675,
        ],
        std=[
            1.0,
            1.0,
            1.0,
        ],
        to_rgb=False,
        type='NormalizeMultiviewImage'),
    dict(type='DefaultFormatBundle'),
    dict(num_cams=6, type='NuScenesAdaptor', use_ego=True),
]
val_dataset_config = dict(
    data_aug_conf=dict(
        H=900,
        W=1600,
        bot_pct_lim=(
            0.0,
            0.0,
        ),
        final_dim=(
            896,
            1600,
        ),
        rand_flip=False,
        resize_lim=(
            1.0,
            1.0,
        ),
        rot_lim=(
            0.0,
            0.0,
        )),
    data_root='data/nuscenes/',
    imageset='data/bevdetv2-nuscenes_infos_val.pkl',
    is_mini=False,
    phase='val',
    pipeline=[
        dict(to_float32=True, type='LoadMultiViewImageFromFiles'),
        dict(occ_path='./data/gts/', type='LoadOccupancyOcc3dNuscenes'),
        dict(type='ResizeCropFlipImage'),
        dict(
            mean=[
                103.53,
                116.28,
                123.675,
            ],
            std=[
                1.0,
                1.0,
                1.0,
            ],
            to_rgb=False,
            type='NormalizeMultiviewImage'),
        dict(type='DefaultFormatBundle'),
        dict(num_cams=6, type='NuScenesAdaptor', use_ego=True),
    ],
    type='Occ3dNuScenesDataset')
val_loader = dict(batch_size=1, num_workers=8, shuffle=False)
val_pipeline = [
    dict(to_float32=True, type='LoadMultiViewImageFromFiles'),
    dict(occ_path='./data/gts/', type='LoadOccupancyOcc3dNuscenes'),
    dict(type='ResizeCropFlipImage'),
    dict(
        mean=[
            103.53,
            116.28,
            123.675,
        ],
        std=[
            1.0,
            1.0,
            1.0,
        ],
        to_rgb=False,
        type='NormalizeMultiviewImage'),
    dict(type='DefaultFormatBundle'),
    dict(num_cams=6, type='NuScenesAdaptor', use_ego=True),
]
voxel_size = 0.4
vx = 0.4
work_dir = 'out/eval_local/occ3d/tpv/onlycam/version_1'
xmax = 40
xmin = -40
ymax = 40
ymin = -40
zmax = 5.4
zmin = -1

Name of parameter - Initialization information

img_backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer1.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer1.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.0.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 

img_backbone.layer1.0.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer1.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer1.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.1.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 

img_backbone.layer1.1.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer1.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer1.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.2.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 

img_backbone.layer1.2.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer2.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer2.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.0.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 

img_backbone.layer2.0.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer2.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer2.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.1.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 

img_backbone.layer2.1.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer2.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer2.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.2.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 

img_backbone.layer2.2.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.3.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer2.3.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.3.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer2.3.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.3.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 

img_backbone.layer2.3.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.0.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.0.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.0.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.1.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.1.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.1.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.2.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.2.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.2.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.2.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.2.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.2.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.2.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.3.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.3.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.3.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.3.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.3.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.3.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.3.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.3.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.4.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.4.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.4.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.4.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.4.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.4.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.4.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.4.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.5.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.5.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.5.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.5.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.5.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.5.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.5.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.5.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.6.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.6.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.6.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.6.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.6.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.6.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.6.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.6.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.6.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.6.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.6.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.7.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.7.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.7.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.7.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.7.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.7.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.7.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.7.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.7.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.7.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.7.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.8.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.8.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.8.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.8.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.8.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.8.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.8.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.8.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.8.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.8.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.8.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.9.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.9.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.9.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.9.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.9.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.9.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.9.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.9.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.9.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.9.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.9.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.10.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.10.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.10.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.10.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.10.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.10.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.10.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.10.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.10.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.10.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.10.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.11.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.11.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.11.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.11.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.11.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.11.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.11.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.11.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.11.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.11.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.11.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.12.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.12.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.12.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.12.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.12.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.12.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.12.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.12.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.12.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.12.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.12.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.13.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.13.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.13.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.13.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.13.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.13.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.13.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.13.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.13.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.13.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.13.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.14.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.14.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.14.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.14.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.14.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.14.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.14.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.14.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.14.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.14.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.14.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.15.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.15.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.15.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.15.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.15.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.15.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.15.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.15.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.15.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.15.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.15.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.16.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.16.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.16.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.16.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.16.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.16.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.16.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.16.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.16.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.16.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.16.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.17.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.17.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.17.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.17.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.17.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.17.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.17.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.17.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.17.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.17.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.17.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.18.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.18.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.18.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.18.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.18.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.18.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.18.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.18.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.18.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.18.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.18.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.19.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.19.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.19.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.19.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.19.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.19.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.19.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.19.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.19.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.19.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.19.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.20.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.20.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.20.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.20.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.20.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.20.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.20.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.20.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.20.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.20.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.20.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.21.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.21.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.21.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.21.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.21.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.21.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.21.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.21.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.21.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.21.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.21.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.22.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.22.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.22.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.22.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.22.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.22.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.22.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.22.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer3.22.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.22.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.22.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.0.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer4.0.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.0.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer4.0.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer4.0.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.0.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 

img_backbone.layer4.0.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.1.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer4.1.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.1.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer4.1.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer4.1.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.1.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 

img_backbone.layer4.1.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.2.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer4.2.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.2.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer4.2.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer4.2.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.2.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 

img_backbone.layer4.2.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_neck.lateral_convs.0.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_neck.lateral_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_neck.lateral_convs.2.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

img_neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in TPVFormerHead  

head.cams_embeds - torch.Size([6, 256]): 
Initialized by user-defined `init_weights` in TPVFormerHead  

head.positional_encoding.row_embed.weight - torch.Size([100, 128]): 
Initialized by user-defined `init_weights` in TPVFormerHead  

head.positional_encoding.col_embed.weight - torch.Size([100, 128]): 
Initialized by user-defined `init_weights` in TPVFormerHead  

head.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in TPVFormerHead  

head.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in TPVFormerHead  

head.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.0.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.1.weight - torch.Size([4096, 256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.2.weight - torch.Size([4096, 256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.2.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.0.attentions.1.deformable_attention.attention_weights.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.0.attentions.1.deformable_attention.attention_weights.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.0.attentions.1.deformable_attention.attention_weights.1.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.0.attentions.1.deformable_attention.attention_weights.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.0.attentions.1.deformable_attention.attention_weights.2.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.0.attentions.1.deformable_attention.attention_weights.2.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.0.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in TPVFormerHead  

head.encoder.layers.0.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in TPVFormerHead  

head.encoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in TPVFormerHead  

head.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in TPVFormerHead  

head.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in TPVFormerHead  

head.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in TPVFormerHead  

head.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.0.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.1.weight - torch.Size([4096, 256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.2.weight - torch.Size([4096, 256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.2.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.1.attentions.1.deformable_attention.attention_weights.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.1.attentions.1.deformable_attention.attention_weights.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.1.attentions.1.deformable_attention.attention_weights.1.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.1.attentions.1.deformable_attention.attention_weights.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.1.attentions.1.deformable_attention.attention_weights.2.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.1.attentions.1.deformable_attention.attention_weights.2.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.1.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in TPVFormerHead  

head.encoder.layers.1.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in TPVFormerHead  

head.encoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in TPVFormerHead  

head.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in TPVFormerHead  

head.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in TPVFormerHead  

head.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in TPVFormerHead  

head.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.0.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.1.weight - torch.Size([4096, 256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.2.weight - torch.Size([4096, 256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.2.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.2.attentions.1.deformable_attention.attention_weights.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.2.attentions.1.deformable_attention.attention_weights.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.2.attentions.1.deformable_attention.attention_weights.1.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.2.attentions.1.deformable_attention.attention_weights.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.2.attentions.1.deformable_attention.attention_weights.2.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.2.attentions.1.deformable_attention.attention_weights.2.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.2.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in TPVFormerHead  

head.encoder.layers.2.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in TPVFormerHead  

head.encoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in TPVFormerHead  

head.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in TPVFormerHead  

head.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.encoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

head.tpv_embedding_hw.weight - torch.Size([10000, 256]): 
Initialized by user-defined `init_weights` in TPVFormerHead  

head.tpv_embedding_zh.weight - torch.Size([800, 256]): 
Initialized by user-defined `init_weights` in TPVFormerHead  

head.tpv_embedding_wz.weight - torch.Size([800, 256]): 
Initialized by user-defined `init_weights` in TPVFormerHead  

aggregator.decoder.0.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

aggregator.decoder.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

aggregator.decoder.2.weight - torch.Size([256, 512]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

aggregator.decoder.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

aggregator.classifier.weight - torch.Size([18, 256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

aggregator.classifier.bias - torch.Size([18]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

aggregator.renderer_prep.decoder.mlp_coarse.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

aggregator.renderer_prep.decoder.mlp_coarse.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

aggregator.renderer_prep.decoder.mlp_coarse.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

aggregator.renderer_prep.decoder.mlp_coarse.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

aggregator.renderer_prep.decoder.mlp_coarse.4.weight - torch.Size([29, 256]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  

aggregator.renderer_prep.decoder.mlp_coarse.4.bias - torch.Size([29]): 
The value is the same before and after calling `init_weights` of BEVSegmentor  
2025/02/28 09:46:37 - mmengine - INFO - Number of params: 62604589
2025/02/28 09:46:38 - mmengine - INFO - resume from: ckpts/final/occ3d_tpv_onlycam.pth
2025/02/28 09:46:38 - mmengine - INFO - work dir: out/eval_local/occ3d/tpv/onlycam
2025/02/28 09:46:38 - mmengine - INFO - ModulatedDeformConvPack img_backbone.layer3.0.conv2 is upgraded to version 2.
2025/02/28 09:46:38 - mmengine - INFO - ModulatedDeformConvPack img_backbone.layer3.1.conv2 is upgraded to version 2.
2025/02/28 09:46:38 - mmengine - INFO - ModulatedDeformConvPack img_backbone.layer3.2.conv2 is upgraded to version 2.
2025/02/28 09:46:38 - mmengine - INFO - ModulatedDeformConvPack img_backbone.layer3.3.conv2 is upgraded to version 2.
2025/02/28 09:46:38 - mmengine - INFO - ModulatedDeformConvPack img_backbone.layer3.4.conv2 is upgraded to version 2.
2025/02/28 09:46:38 - mmengine - INFO - ModulatedDeformConvPack img_backbone.layer3.5.conv2 is upgraded to version 2.
2025/02/28 09:46:38 - mmengine - INFO - ModulatedDeformConvPack img_backbone.layer3.6.conv2 is upgraded to version 2.
2025/02/28 09:46:38 - mmengine - INFO - ModulatedDeformConvPack img_backbone.layer3.7.conv2 is upgraded to version 2.
2025/02/28 09:46:38 - mmengine - INFO - ModulatedDeformConvPack img_backbone.layer3.8.conv2 is upgraded to version 2.
2025/02/28 09:46:38 - mmengine - INFO - ModulatedDeformConvPack img_backbone.layer3.9.conv2 is upgraded to version 2.
2025/02/28 09:46:38 - mmengine - INFO - ModulatedDeformConvPack img_backbone.layer3.10.conv2 is upgraded to version 2.
2025/02/28 09:46:38 - mmengine - INFO - ModulatedDeformConvPack img_backbone.layer3.11.conv2 is upgraded to version 2.
2025/02/28 09:46:38 - mmengine - INFO - ModulatedDeformConvPack img_backbone.layer3.12.conv2 is upgraded to version 2.
2025/02/28 09:46:38 - mmengine - INFO - ModulatedDeformConvPack img_backbone.layer3.13.conv2 is upgraded to version 2.
2025/02/28 09:46:38 - mmengine - INFO - ModulatedDeformConvPack img_backbone.layer3.14.conv2 is upgraded to version 2.
2025/02/28 09:46:38 - mmengine - INFO - ModulatedDeformConvPack img_backbone.layer3.15.conv2 is upgraded to version 2.
2025/02/28 09:46:38 - mmengine - INFO - ModulatedDeformConvPack img_backbone.layer3.16.conv2 is upgraded to version 2.
2025/02/28 09:46:38 - mmengine - INFO - ModulatedDeformConvPack img_backbone.layer3.17.conv2 is upgraded to version 2.
2025/02/28 09:46:38 - mmengine - INFO - ModulatedDeformConvPack img_backbone.layer3.18.conv2 is upgraded to version 2.
2025/02/28 09:46:38 - mmengine - INFO - ModulatedDeformConvPack img_backbone.layer3.19.conv2 is upgraded to version 2.
2025/02/28 09:46:38 - mmengine - INFO - ModulatedDeformConvPack img_backbone.layer3.20.conv2 is upgraded to version 2.
2025/02/28 09:46:38 - mmengine - INFO - ModulatedDeformConvPack img_backbone.layer3.21.conv2 is upgraded to version 2.
2025/02/28 09:46:38 - mmengine - INFO - ModulatedDeformConvPack img_backbone.layer3.22.conv2 is upgraded to version 2.
2025/02/28 09:46:38 - mmengine - INFO - ModulatedDeformConvPack img_backbone.layer4.0.conv2 is upgraded to version 2.
2025/02/28 09:46:38 - mmengine - INFO - ModulatedDeformConvPack img_backbone.layer4.1.conv2 is upgraded to version 2.
2025/02/28 09:46:38 - mmengine - INFO - ModulatedDeformConvPack img_backbone.layer4.2.conv2 is upgraded to version 2.
2025/02/28 09:46:38 - mmengine - INFO - successfully resumed from epoch 20
2025/02/28 09:46:38 - mmengine - INFO - load from: ckpts/r101_dcn_fcos3d_pretrain.pth
2025/02/28 09:46:38 - mmengine - INFO - resume from: ckpts/final/occ3d_tpv_onlycam.pth
2025/02/28 09:46:44 - mmengine - INFO - [EVAL] Iter     0
2025/02/28 09:48:03 - mmengine - INFO - [EVAL] Iter   250
2025/02/28 09:49:23 - mmengine - INFO - [EVAL] Iter   500
2025/02/28 09:50:41 - mmengine - INFO - [EVAL] Iter   750
2025/02/28 09:52:00 - mmengine - INFO - [EVAL] Iter  1000
2025/02/28 09:53:20 - mmengine - INFO - [EVAL] Iter  1250
2025/02/28 09:54:40 - mmengine - INFO - [EVAL] Iter  1500
2025/02/28 09:56:00 - mmengine - INFO - [EVAL] Iter  1750
2025/02/28 09:57:19 - mmengine - INFO - [EVAL] Iter  2000
2025/02/28 09:58:38 - mmengine - INFO - [EVAL] Iter  2250
2025/02/28 09:59:57 - mmengine - INFO - [EVAL] Iter  2500
2025/02/28 10:01:16 - mmengine - INFO - [EVAL] Iter  2750
2025/02/28 10:02:36 - mmengine - INFO - [EVAL] Iter  3000
2025/02/28 10:03:56 - mmengine - INFO - [EVAL] Iter  3250
2025/02/28 10:05:15 - mmengine - INFO - [EVAL] Iter  3500
2025/02/28 10:06:35 - mmengine - INFO - [EVAL] Iter  3750
2025/02/28 10:07:55 - mmengine - INFO - [EVAL] Iter  4000
2025/02/28 10:09:14 - mmengine - INFO - [EVAL] Iter  4250
2025/02/28 10:10:32 - mmengine - INFO - [EVAL] Iter  4500
2025/02/28 10:11:51 - mmengine - INFO - [EVAL] Iter  4750
2025/02/28 10:13:10 - mmengine - INFO - [EVAL] Iter  5000
2025/02/28 10:14:29 - mmengine - INFO - [EVAL] Iter  5250
2025/02/28 10:16:26 - mmengine - INFO - [EVAL] Iter  5500
2025/02/28 10:17:44 - mmengine - INFO - [EVAL] Iter  5750
2025/02/28 10:19:03 - mmengine - INFO - [EVAL] Iter  6000
2025/02/28 10:19:08 - mmengine - INFO - 
IoU img:
2025/02/28 10:19:08 - mmengine - INFO - Validation per class iou none:
2025/02/28 10:19:08 - mmengine - INFO - other : 0.00%, 0.00, 0.00
2025/02/28 10:19:08 - mmengine - INFO - barrier : 69.08%, 0.80, 0.83
2025/02/28 10:19:08 - mmengine - INFO - bicycle : 0.00%, 0.00, 0.00
2025/02/28 10:19:08 - mmengine - INFO - bus : 79.98%, 0.92, 0.86
2025/02/28 10:19:08 - mmengine - INFO - car : 79.61%, 0.89, 0.89
2025/02/28 10:19:08 - mmengine - INFO - construction_vehicle : 42.66%, 0.57, 0.63
2025/02/28 10:19:08 - mmengine - INFO - motorcycle : 42.12%, 0.71, 0.51
2025/02/28 10:19:08 - mmengine - INFO - pedestrian : 66.68%, 0.80, 0.80
2025/02/28 10:19:08 - mmengine - INFO - traffic_cone : 31.49%, 0.55, 0.43
2025/02/28 10:19:08 - mmengine - INFO - trailer : 35.96%, 0.47, 0.60
2025/02/28 10:19:08 - mmengine - INFO - truck : 68.80%, 0.80, 0.83
2025/02/28 10:19:08 - mmengine - INFO - driveable_surface : 88.14%, 0.93, 0.94
2025/02/28 10:19:08 - mmengine - INFO - other_flat : 51.75%, 0.72, 0.65
2025/02/28 10:19:08 - mmengine - INFO - sidewalk : 50.96%, 0.68, 0.67
2025/02/28 10:19:08 - mmengine - INFO - terrain : 53.59%, 0.73, 0.67
2025/02/28 10:19:08 - mmengine - INFO - manmade : 72.12%, 0.83, 0.85
2025/02/28 10:19:08 - mmengine - INFO - vegetation : 72.04%, 0.84, 0.84
2025/02/28 10:19:08 - mmengine - INFO - empty : 84.35%, 0.91, 0.92
2025/02/28 10:19:08 - mmengine - INFO - -------------------------
2025/02/28 10:19:08 - mmengine - INFO - mIoU w. empty cls : 54.96
2025/02/28 10:19:08 - mmengine - INFO - mIoU wo. empty cls : 53.23
2025/02/28 10:19:08 - mmengine - INFO - IoU as non-empty IoU: 94.34
2025/02/28 10:19:08 - mmengine - INFO - Validation L1 loss: 1.3037795844124507
2025/02/28 10:19:08 - mmengine - INFO - L1 Depth: 1.3037795844124507
2025/02/28 10:19:08 - mmengine - INFO - IoU BeV:
2025/02/28 10:19:08 - mmengine - INFO - Validation per class iou none:
2025/02/28 10:19:08 - mmengine - INFO - other : 0.00%, 0.00, 0.00
2025/02/28 10:19:08 - mmengine - INFO - barrier : 26.92%, 0.49, 0.37
2025/02/28 10:19:08 - mmengine - INFO - bicycle : 0.00%, 0.00, 0.00
2025/02/28 10:19:08 - mmengine - INFO - bus : 44.14%, 0.69, 0.55
2025/02/28 10:19:08 - mmengine - INFO - car : 44.89%, 0.66, 0.59
2025/02/28 10:19:08 - mmengine - INFO - construction_vehicle : 11.80%, 0.39, 0.14
2025/02/28 10:19:08 - mmengine - INFO - motorcycle : 12.18%, 0.48, 0.14
2025/02/28 10:19:08 - mmengine - INFO - pedestrian : 22.90%, 0.48, 0.31
2025/02/28 10:19:08 - mmengine - INFO - traffic_cone : 12.28%, 0.31, 0.17
2025/02/28 10:19:08 - mmengine - INFO - trailer : 22.12%, 0.39, 0.34
2025/02/28 10:19:08 - mmengine - INFO - truck : 34.32%, 0.59, 0.45
2025/02/28 10:19:08 - mmengine - INFO - driveable_surface : 60.44%, 0.74, 0.77
2025/02/28 10:19:08 - mmengine - INFO - other_flat : 27.13%, 0.62, 0.33
2025/02/28 10:19:08 - mmengine - INFO - sidewalk : 29.76%, 0.55, 0.39
2025/02/28 10:19:08 - mmengine - INFO - terrain : 34.67%, 0.59, 0.46
2025/02/28 10:19:08 - mmengine - INFO - manmade : 31.67%, 0.53, 0.44
2025/02/28 10:19:08 - mmengine - INFO - vegetation : 41.73%, 0.59, 0.59
2025/02/28 10:19:08 - mmengine - INFO - empty : 64.02%, 0.75, 0.82
2025/02/28 10:19:08 - mmengine - INFO - -------------------------
2025/02/28 10:19:08 - mmengine - INFO - mIoU w. empty cls : 28.94
2025/02/28 10:19:08 - mmengine - INFO - mIoU wo. empty cls : 26.88
2025/02/28 10:19:08 - mmengine - INFO - IoU as non-empty IoU: 64.60
2025/02/28 10:19:08 - mmengine - INFO - IoU 3D:
2025/02/28 10:19:08 - mmengine - INFO - Validation per class iou none:
2025/02/28 10:19:08 - mmengine - INFO - other : 0.00%, 0.00, 0.00
2025/02/28 10:19:08 - mmengine - INFO - barrier : 30.98%, 0.45, 0.50
2025/02/28 10:19:08 - mmengine - INFO - bicycle : 0.00%, 0.00, 0.00
2025/02/28 10:19:08 - mmengine - INFO - bus : 37.66%, 0.51, 0.59
2025/02/28 10:19:08 - mmengine - INFO - car : 41.77%, 0.52, 0.68
2025/02/28 10:19:08 - mmengine - INFO - construction_vehicle : 13.59%, 0.24, 0.24
2025/02/28 10:19:08 - mmengine - INFO - motorcycle : 8.80%, 0.19, 0.14
2025/02/28 10:19:08 - mmengine - INFO - pedestrian : 20.83%, 0.31, 0.38
2025/02/28 10:19:08 - mmengine - INFO - traffic_cone : 11.43%, 0.22, 0.19
2025/02/28 10:19:08 - mmengine - INFO - trailer : 22.84%, 0.34, 0.41
2025/02/28 10:19:08 - mmengine - INFO - truck : 30.84%, 0.43, 0.53
2025/02/28 10:19:08 - mmengine - INFO - driveable_surface : 65.76%, 0.89, 0.71
2025/02/28 10:19:08 - mmengine - INFO - other_flat : 24.25%, 0.68, 0.27
2025/02/28 10:19:08 - mmengine - INFO - sidewalk : 36.66%, 0.71, 0.43
2025/02/28 10:19:08 - mmengine - INFO - terrain : 38.16%, 0.67, 0.47
2025/02/28 10:19:08 - mmengine - INFO - manmade : 24.47%, 0.29, 0.59
2025/02/28 10:19:08 - mmengine - INFO - vegetation : 22.45%, 0.25, 0.67
2025/02/28 10:19:08 - mmengine - INFO - empty : 75.39%, 0.92, 0.81
2025/02/28 10:19:08 - mmengine - INFO - -------------------------
2025/02/28 10:19:08 - mmengine - INFO - mIoU w. empty cls : 28.10
2025/02/28 10:19:08 - mmengine - INFO - mIoU wo. empty cls : 25.32
2025/02/28 10:19:08 - mmengine - INFO - IoU as non-empty IoU: 45.48
