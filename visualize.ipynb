{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import get_dataset\n",
    "from mmengine import Config\n",
    "import plotly.express as px\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import torch\n",
    "from src.utils.visualisation import colors, get_3d_corners\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from src.model.utils.utils import modify_file_inplace\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "modify_file_inplace(\"./config/_base_/data_select.py\", \"occ3d\")\n",
    "cfg = Config.fromfile(\n",
    "    \"./config/tpvformer/render.py\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import get_dataloader\n",
    "\n",
    "train_dataset_loader, val_dataset_loader = get_dataloader(\n",
    "    cfg.train_dataset_config,\n",
    "    cfg.val_dataset_config,\n",
    "    cfg.train_loader,\n",
    "    cfg.val_loader,\n",
    "    dist=None,\n",
    "    iter_resume=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.model\n",
    "from mmseg.models import build_segmentor\n",
    "from IPython.display import clear_output\n",
    "\n",
    "my_model = build_segmentor(cfg.model)\n",
    "my_model.init_weights()\n",
    "my_model.cuda()\n",
    "\n",
    "# Checkpoint\n",
    "my_model.load_state_dict(\n",
    "    torch.load(\"./ckpts/final/occ3d_tpv_std.pth\")[\"state_dict\"], strict=False\n",
    ")\n",
    "my_model.eval()\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a random index\n",
    "for i_iter, data in enumerate(val_dataset_loader):\n",
    "    if i_iter == 0:\n",
    "        break\n",
    "    \n",
    "# Select a specific index\n",
    "# from dataset import OPENOCC_DATASET, get_dataloader\n",
    "# from dataset.utils import custom_collate_fn_temporal\n",
    "# val_wrapper = OPENOCC_DATASET.build(cfg.val_dataset_config)\n",
    "# data = custom_collate_fn_temporal([val_wrapper[0]])\n",
    "\n",
    "for k in list(data.keys()):\n",
    "    if isinstance(data[k], torch.Tensor):\n",
    "        data[k] = data[k].cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "resize_factor = 0.5  # Adjust this factor for faster display\n",
    "\n",
    "# Permute to (B, H, W, C) and reverse the color channels from RGB to BGR\n",
    "mean = torch.tensor(cfg.val_dataset_config[\"pipeline\"][3][\"mean\"])\n",
    "std = torch.tensor(cfg.val_dataset_config[\"pipeline\"][3][\"std\"])\n",
    "\n",
    "# Permute to (B, H, W, C) and reverse the color channels from RGB to BGR\n",
    "images = data[\"img\"][0].permute(0, 2, 3, 1).cpu()\n",
    "\n",
    "# Unnormalize the images\n",
    "images = ((images * std + mean) / 255.0).clip(0, 1)\n",
    "images = images[..., [2, 1, 0]]\n",
    "\n",
    "# Resize the images for faster display\n",
    "new_size = [int(images.shape[1] * resize_factor), int(images.shape[2] * resize_factor)]\n",
    "images = F.interpolate(images.permute(0, 3, 1, 2), size=new_size, mode=\"bilinear\", align_corners=False)\n",
    "images = images.permute(0, 2, 3, 1)  # Convert back to (B, H, W, C)\n",
    "\n",
    "# Create a grid plot\n",
    "num_images = images.shape[0]\n",
    "fig, axes = plt.subplots(2, 3, figsize=(10, 4))\n",
    "images = images[[1,0,2,4,3,5]]\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < num_images:\n",
    "        ax.imshow(images[i].numpy())\n",
    "        ax.axis(\"off\")  # Hide axes for cleaner visualization\n",
    "    else:\n",
    "        ax.axis(\"off\")  # Hide unused subplots\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rendering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customize renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Camera location\n",
    "my_model.renderer.render_gt_mode = \"sensor\"\n",
    "# Number of cameras\n",
    "my_model.renderer.render_ncam = 6\n",
    "# Specify a camera index\n",
    "my_model.renderer.cam_idx = [0, 1, 2, 3, 4, 5]\n",
    "# Dataset tag\n",
    "my_model.renderer.dataset_tag = cfg.dataset_tag\n",
    "# Gaussian scale\n",
    "gaussian_scale = 0.27\n",
    "my_model.renderer.gaussian_scale = gaussian_scale\n",
    "my_model.aggregator.renderer_prep.gaussian_scale = gaussian_scale\n",
    "my_model.aggregator.renderer_prep.overwrite_opacity = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from src.model.utils.utils import (\n",
    "    set_gt_render,\n",
    "    set_matrix_to_render,\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    set_matrix_to_render(False, my_model, data)\n",
    "\n",
    "    result_dict = my_model(imgs=data[\"img\"], metas=data)\n",
    "\n",
    "    set_gt_render(False, my_model, data, result_dict[\"render\"][\"valid\"])\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot GT & Pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import torch\n",
    "from skimage.transform import resize  # Import resize function for rescaling\n",
    "\n",
    "# Example camera labels\n",
    "camera_labels = [\n",
    "    \"CAM_FRONT\",\n",
    "    \"CAM_FRONT_RIGHT\",\n",
    "    \"CAM_FRONT_LEFT\",\n",
    "    \"CAM_BACK\",\n",
    "    \"CAM_BACK_RIGHT\",\n",
    "    \"CAM_BACK_LEFT\",\n",
    "]\n",
    "\n",
    "# Set the factor for rescaling. For example, 2 will reduce the size by half.\n",
    "factor = 4\n",
    "rescale = True  # Set this to True to enable rescaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_arrays_gt = []\n",
    "img_arrays_pred = []\n",
    "\n",
    "# Assuming `data[\"render_cam\"]` is ground truth and `result_dict[\"render\"][\"cam\"]` is predictions\n",
    "for i in range(6):\n",
    "    # Ground truth rendering\n",
    "    render_cam = data[\"render_cam\"][0, i]\n",
    "    map_colors = torch.index_select(\n",
    "        torch.from_numpy(colors).cuda(),\n",
    "        0,\n",
    "        render_cam.argmax(-1).flatten().to(torch.int32),\n",
    "    )[:, :3]\n",
    "\n",
    "    gt_mapped_img = (\n",
    "        (map_colors.reshape(*render_cam.shape[:2], 3) / 255).cpu().numpy()\n",
    "    )\n",
    "    \n",
    "    # Rescale if enabled\n",
    "    if rescale:\n",
    "        gt_mapped_img_rescaled = resize(gt_mapped_img, \n",
    "                                        (gt_mapped_img.shape[0] // factor, \n",
    "                                         gt_mapped_img.shape[1] // factor), \n",
    "                                        anti_aliasing=True)\n",
    "        img_arrays_gt.append(gt_mapped_img_rescaled)  # Append ground truth image\n",
    "    else:\n",
    "        img_arrays_gt.append(gt_mapped_img)  # Append ground truth image without scaling\n",
    "\n",
    "    # Prediction rendering\n",
    "    pred_cam = result_dict[\"render\"][\"cam\"][0, i]  # Predictions\n",
    "    pred_map_colors = torch.index_select(\n",
    "        torch.from_numpy(colors).cuda(),\n",
    "        0,\n",
    "        pred_cam.argmax(-1).flatten().to(torch.int32),\n",
    "    )[:, :3]\n",
    "\n",
    "    pred_mapped_img = (\n",
    "        (pred_map_colors.reshape(*pred_cam.shape[:2], 3) / 255)\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "    )\n",
    "\n",
    "    # Rescale if enabled\n",
    "    if rescale:\n",
    "        pred_mapped_img_rescaled = resize(pred_mapped_img, \n",
    "                                          (pred_mapped_img.shape[0] // factor, \n",
    "                                           pred_mapped_img.shape[1] // factor), \n",
    "                                          anti_aliasing=True)\n",
    "        img_arrays_pred.append(pred_mapped_img_rescaled)  # Append predicted image\n",
    "    else:\n",
    "        img_arrays_pred.append(pred_mapped_img)  # Append predicted image without scaling\n",
    "\n",
    "# Plot both ground truth and predictions in separate subplots\n",
    "fig = px.imshow(\n",
    "    np.stack(img_arrays_gt),\n",
    "    facet_col=0,\n",
    "    facet_col_wrap=3,\n",
    "    title=\"Ground Truth: Plot all sensor images\",\n",
    ")\n",
    "fig.update_layout(title=\"Ground Truth: Plot all sensor images\")\n",
    "\n",
    "# Replace facet titles with camera labels\n",
    "fig.for_each_annotation(\n",
    "    lambda a: a.update(text=camera_labels[int(a.text.split(\"=\")[-1])])\n",
    ")\n",
    "\n",
    "# Create a new figure for the predictions\n",
    "fig_pred = px.imshow(\n",
    "    np.stack(img_arrays_pred),\n",
    "    facet_col=0,\n",
    "    facet_col_wrap=3,\n",
    "    title=\"Predictions: Plot all sensor images\",\n",
    ")\n",
    "fig_pred.update_layout(title=\"Predictions: Plot all sensor images\")\n",
    "\n",
    "# Replace facet titles with camera labels\n",
    "fig_pred.for_each_annotation(\n",
    "    lambda a: a.update(text=camera_labels[int(a.text.split(\"=\")[-1])])\n",
    ")\n",
    "\n",
    "# Show the ground truth and predictions\n",
    "fig.show()\n",
    "fig_pred.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.utils.utils import set_up_metrics\n",
    "\n",
    "_, miou_metric_img, *_ = set_up_metrics(cfg.dataset_tag)\n",
    "\n",
    "np.mean(\n",
    "    miou_metric_img._compute_iou(\n",
    "        data[\"render_cam\"].argmax(-1).flatten(),\n",
    "        result_dict[\"render\"][\"cam\"].argmax(-1).flatten(),\n",
    "        None,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `data[\"render_cam_depth\"]` is ground truth depth and `result_dict[\"render\"][\"depth\"]` is predicted depth\n",
    "depth_arrays_gt = []\n",
    "depth_arrays_pred = []\n",
    "\n",
    "# Iterate through each camera for ground truth and prediction depth\n",
    "for i in range(6):\n",
    "    # Ground truth depth rendering\n",
    "    render_cam_depth = data[\"render_cam_depth\"][0, i].cpu().numpy()\n",
    "\n",
    "    # Rescale if enabled\n",
    "    if rescale:\n",
    "        gt_depth_rescaled = resize(render_cam_depth,\n",
    "                                   (render_cam_depth.shape[0] // factor,\n",
    "                                    render_cam_depth.shape[1] // factor),\n",
    "                                   anti_aliasing=True)\n",
    "        depth_arrays_gt.append(gt_depth_rescaled)  # Append rescaled ground truth depth\n",
    "    else:\n",
    "        depth_arrays_gt.append(render_cam_depth)  # Append original ground truth depth\n",
    "\n",
    "    # Prediction depth rendering\n",
    "    pred_cam_depth = result_dict[\"render\"][\"depth\"][0, i].cpu().numpy()  # Predictions\n",
    "\n",
    "    # Rescale if enabled\n",
    "    if rescale:\n",
    "        pred_depth_rescaled = resize(pred_cam_depth,\n",
    "                                     (pred_cam_depth.shape[0] // factor,\n",
    "                                      pred_cam_depth.shape[1] // factor),\n",
    "                                     anti_aliasing=True)\n",
    "        depth_arrays_pred.append(pred_depth_rescaled)  # Append rescaled predicted depth\n",
    "    else:\n",
    "        depth_arrays_pred.append(pred_cam_depth)  # Append original predicted depth\n",
    "\n",
    "# Plot both ground truth and predictions in separate subplots for depth images\n",
    "fig_gt = px.imshow(\n",
    "    np.array(depth_arrays_gt).squeeze(-1),\n",
    "    facet_col=0,\n",
    "    facet_col_wrap=3,\n",
    "    title=\"Ground Truth:\",\n",
    ")\n",
    "fig_gt.update_layout(title=\"Ground Truth: Plot all depths\")\n",
    "\n",
    "# Replace facet titles with camera labels\n",
    "fig_gt.for_each_annotation(\n",
    "    lambda a: a.update(text=camera_labels[int(a.text.split(\"=\")[-1])])\n",
    ")\n",
    "\n",
    "# Create a new figure for the predictions\n",
    "fig_pred = px.imshow(\n",
    "    np.array(depth_arrays_pred).squeeze(-1),\n",
    "    facet_col=0,\n",
    "    facet_col_wrap=3,\n",
    "    title=\"Predictions:\",\n",
    ")\n",
    "fig_pred.update_layout(title=\"Predictions: Plot all depths\")\n",
    "\n",
    "# Replace facet titles with camera labels\n",
    "fig_pred.for_each_annotation(\n",
    "    lambda a: a.update(text=camera_labels[int(a.text.split(\"=\")[-1])])\n",
    ")\n",
    "\n",
    "# Show the ground truth and predictions\n",
    "fig_gt.show()\n",
    "fig_pred.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BeV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example BEV labels (for BEV, you might not need camera labels)\n",
    "bev_labels = [\n",
    "    \"Ground Truth BEV\",\n",
    "    \"Predicted BEV\",\n",
    "]\n",
    "\n",
    "# Set the factor for rescaling. For example, 4 will reduce the size by a quarter.\n",
    "factor = 1\n",
    "rescale = True  # Set this to True to enable rescaling\n",
    "\n",
    "# Assuming `data[\"render_bev\"]` is ground truth BEV and `result_dict[\"render\"][\"bev\"]` is predicted BEV\n",
    "bev_arrays_gt = []\n",
    "bev_arrays_pred = []\n",
    "\n",
    "# Ground truth BEV rendering\n",
    "render_bev_gt = data[\"render_bev\"][0].cpu()\n",
    "sem_cls = render_bev_gt.argmax(-1).flatten().to(torch.int32)\n",
    "if cfg.dataset_tag == \"occ3d\":\n",
    "    sem_cls[sem_cls == 17] = 20\n",
    "map_colors_gt = torch.index_select(\n",
    "    torch.from_numpy(colors),\n",
    "    0,\n",
    "    sem_cls,\n",
    ")[:, :3]\n",
    "\n",
    "gt_mapped_bev = (map_colors_gt.reshape(*render_bev_gt.shape[:2], 3) / 255).cpu().numpy()\n",
    "\n",
    "# Rescale if enabled\n",
    "if rescale:\n",
    "    gt_mapped_bev_rescaled = resize(\n",
    "        gt_mapped_bev,\n",
    "        (gt_mapped_bev.shape[0] // factor, gt_mapped_bev.shape[1] // factor),\n",
    "        anti_aliasing=True,\n",
    "    )\n",
    "    bev_arrays_gt.append(gt_mapped_bev_rescaled)  # Append rescaled ground truth BEV\n",
    "else:\n",
    "    bev_arrays_gt.append(gt_mapped_bev)  # Append original ground truth BEV\n",
    "\n",
    "# Prediction BEV rendering\n",
    "render_bev_pred = result_dict[\"render\"][\"bev\"][0].cpu()  # Predictions\n",
    "sem_cls = render_bev_pred.argmax(-1).flatten().to(torch.int32)\n",
    "if cfg.dataset_tag == \"occ3d\":\n",
    "    sem_cls[sem_cls == 17] = 20\n",
    "pred_map_colors = torch.index_select(\n",
    "    torch.from_numpy(colors),\n",
    "    0,\n",
    "    sem_cls,\n",
    ")[:, :3]\n",
    "\n",
    "pred_mapped_bev = (\n",
    "    (pred_map_colors.reshape(*render_bev_pred.shape[:2], 3) / 255).cpu().numpy()\n",
    ")\n",
    "\n",
    "# Rescale if enabled\n",
    "if rescale:\n",
    "    pred_mapped_bev_rescaled = resize(\n",
    "        pred_mapped_bev,\n",
    "        (pred_mapped_bev.shape[0] // factor, pred_mapped_bev.shape[1] // factor),\n",
    "        anti_aliasing=True,\n",
    "    )\n",
    "    bev_arrays_pred.append(pred_mapped_bev_rescaled)  # Append rescaled predicted BEV\n",
    "else:\n",
    "    bev_arrays_pred.append(pred_mapped_bev)  # Append original predicted BEV\n",
    "\n",
    "# Plot both ground truth and predictions in separate subplots for BEV images\n",
    "fig_gt = px.imshow(\n",
    "    np.concatenate([np.array(bev_arrays_gt), np.array(bev_arrays_pred)]),\n",
    "    facet_col=0,\n",
    "    title=\"Ground Truth BEV\",\n",
    ")\n",
    "fig_gt.update_layout(title=\"Ground Truth / Predicted BEV\")\n",
    "\n",
    "# Replace facet titles with BEV labels\n",
    "fig_gt.for_each_annotation(\n",
    "    lambda a: a.update(text=bev_labels[int(a.text.split(\"=\")[-1])])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BeV depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import torch\n",
    "from skimage.transform import resize  # Import resize function for rescaling\n",
    "\n",
    "# Example BEV labels (for BEV, you might not need camera labels)\n",
    "bev_labels = [\n",
    "    \"Ground Truth BEV\",\n",
    "    \"Predicted BEV\",\n",
    "]\n",
    "\n",
    "# Set the factor for rescaling. For example, 4 will reduce the size by a quarter.\n",
    "factor = 1\n",
    "rescale = True  # Set this to True to enable rescaling\n",
    "\n",
    "# Assuming `data[\"render_bev\"]` is ground truth BEV and `result_dict[\"render\"][\"bev\"]` is predicted BEV\n",
    "bev_arrays_gt = []\n",
    "bev_arrays_pred = []\n",
    "\n",
    "# Ground truth BEV rendering\n",
    "render_bev_gt = data[\"render_bev_depth\"][0].cpu().numpy()\n",
    "\n",
    "# Normalize the depth values to the range [0, 1]\n",
    "gt_normalized_bev = render_bev_gt\n",
    "\n",
    "# Rescale if enabled\n",
    "if rescale:\n",
    "    gt_normalized_bev_rescaled = resize(gt_normalized_bev,\n",
    "                                        (gt_normalized_bev.shape[0] // factor,\n",
    "                                         gt_normalized_bev.shape[1] // factor),\n",
    "                                        anti_aliasing=True)\n",
    "    bev_arrays_gt.append(gt_normalized_bev_rescaled)  # Append rescaled ground truth BEV\n",
    "else:\n",
    "    bev_arrays_gt.append(gt_normalized_bev)  # Append original ground truth BEV\n",
    "\n",
    "# Prediction BEV rendering\n",
    "render_bev_pred = result_dict[\"render\"][\"bev_depth\"][0].cpu().numpy()  # Predictions\n",
    "\n",
    "# Normalize the depth values to the range [0, 1]\n",
    "pred_normalized_bev = (render_bev_pred)\n",
    "\n",
    "# Rescale if enabled\n",
    "if rescale:\n",
    "    pred_normalized_bev_rescaled = resize(pred_normalized_bev,\n",
    "                                          (pred_normalized_bev.shape[0] // factor,\n",
    "                                           pred_normalized_bev.shape[1] // factor),\n",
    "                                          anti_aliasing=True)\n",
    "    bev_arrays_pred.append(pred_normalized_bev_rescaled)  # Append rescaled predicted BEV\n",
    "else:\n",
    "    bev_arrays_pred.append(pred_normalized_bev)  # Append original predicted BEV\n",
    "\n",
    "# Plot both ground truth and predictions in separate subplots for BEV images\n",
    "fig = px.imshow(\n",
    "    np.concatenate([np.array(bev_arrays_gt), np.array(bev_arrays_pred)], axis=0).squeeze(-1),\n",
    "    facet_col=0,\n",
    "    facet_col_wrap=2,\n",
    "    title=\"Ground Truth / Predicted BEV\",\n",
    ")\n",
    "fig.update_layout(title=\"Ground Truth / Predicted BEV\")\n",
    "\n",
    "# Replace facet titles with BEV labels\n",
    "fig.for_each_annotation(\n",
    "    lambda a: a.update(text=bev_labels[int(a.text.split(\"=\")[-1])])\n",
    ")\n",
    "\n",
    "# Show the ground truth and predictions\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render voxels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_xyz, gt_colors = data[\"occ_xyz\"][0].flatten(0, 2), data[\"occ_label\"][0].flatten(0, 2)\n",
    "gt_colors[gt_colors == 255] = 0\n",
    "if cfg.dataset_tag ==\"occ3d\":\n",
    "    empty_cls = 17\n",
    "elif cfg.dataset_tag ==\"surroundocc\":\n",
    "    empty_cls = 0\n",
    "idx = (gt_colors != empty_cls)\n",
    "\n",
    "gt_xyz = gt_xyz[idx]\n",
    "gt_colors = gt_colors[idx]\n",
    "map_colors = torch.index_select(\n",
    "    torch.from_numpy(colors).to(\"cuda\"), 0, gt_colors.to(torch.int32)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify cams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref2cams = torch.cat(\n",
    "    [data[\"render\"][\"ref2cams\"], data[\"render\"][\"ref2cams_bev\"]], dim=1\n",
    ").clone()\n",
    "intrins = torch.cat([data[\"render\"][\"intrins\"], data[\"render\"][\"intrins_bev\"]], dim=1)\n",
    "\n",
    "c2w = torch.linalg.inv(ref2cams)\n",
    "pts = c2w[0, :, :3, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_coeff = 10.0 # Rays depth for camera visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the points\n",
    "fig = go.Figure()\n",
    "Ncam = intrins.shape[1]\n",
    "colors_cam = [\"blue\", \"steelblue\", \"cyan\", \"royalblue\", \"dodgerblue\", \"lightgreen\", \"red\"]\n",
    "\n",
    "# Plot coarse points\n",
    "for _ in range(Ncam-1):\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=pts[_ : _ + 1, 0].cpu(),\n",
    "            y=pts[_ : _ + 1, 1].cpu(),\n",
    "            z=pts[_ : _ + 1, 2].cpu(),\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=3, color=colors_cam[_]),\n",
    "            showlegend=False  # Hide legend for each trace\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Get 3D corners in the world space\n",
    "    H, W = 900, 1600\n",
    "    corners_3d = get_3d_corners(\n",
    "        H, W, intrins[0, _, :3, :3].cpu(), c2w[0, _].cpu(), depth_coeff\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=corners_3d[:, 0],\n",
    "            y=corners_3d[:, 1],\n",
    "            z=corners_3d[:, 2],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=3, color=colors_cam[_]),\n",
    "            showlegend=False  # Hide legend for each trace\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Create lines from camera center to each of the 4 borders\n",
    "    for i in range(4):\n",
    "        fig.add_trace(\n",
    "            go.Scatter3d(\n",
    "                x=np.concatenate([pts[_ : _ + 1, 0].cpu(), corners_3d[i : i + 1, 0]]),\n",
    "                y=np.concatenate([pts[_ : _ + 1, 1].cpu(), corners_3d[i : i + 1, 1]]),\n",
    "                z=np.concatenate([pts[_ : _ + 1, 2].cpu(), corners_3d[i : i + 1, 2]]),\n",
    "                mode=\"lines\",\n",
    "                line=dict(width=3, color=colors_cam[_]),\n",
    "                showlegend=False,  # Hide legend for each trace\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Plot 3D occ\n",
    "fig.add_trace(\n",
    "    go.Scatter3d(\n",
    "        x=gt_xyz[..., 0].cpu(),\n",
    "        y=gt_xyz[..., 1].cpu(),\n",
    "        z=gt_xyz[..., 2].cpu(),\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=1, color=map_colors.cpu()),\n",
    "        showlegend=False,  # Hide legend for this trace too\n",
    "    )\n",
    ")\n",
    "\n",
    "# Set plot layout\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis=dict(visible=False),  # Hide x-axis\n",
    "        yaxis=dict(visible=False),  # Hide y-axis\n",
    "        zaxis=dict(visible=False),  # Hide z-axis\n",
    "        bgcolor=\"white\",  # Set the scene background to white\n",
    "    ),\n",
    "    paper_bgcolor=\"white\",  # Set the entire figure's background to white\n",
    "    plot_bgcolor=\"white\",  # Set the plotting area background to white\n",
    ")\n",
    "fig[\"layout\"][\"scene\"][\"aspectmode\"] = \"data\"\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_segm = result_dict[\"pred_occ\"][-1][0].argmax(0).cpu()\n",
    "pred_xyz = data[\"occ_xyz\"][0].flatten(0, 2).cpu()\n",
    "pred_map_colors = torch.index_select(torch.from_numpy(colors), 0, pred_segm.to(torch.int32))\n",
    "\n",
    "index = (pred_segm != empty_cls) \n",
    "pred_segm = pred_segm[index]\n",
    "pred_xyz = pred_xyz[index]\n",
    "pred_map_colors = pred_map_colors[index]\n",
    "\n",
    "# Plot 3D occ\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter3d(\n",
    "        x=pred_xyz[..., 0].cpu(),\n",
    "        y=pred_xyz[..., 1].cpu(),\n",
    "        z=pred_xyz[..., 2].cpu(),\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=1, color=pred_map_colors.cpu()),\n",
    "        showlegend=False,  # Hide legend for this trace too\n",
    "    )\n",
    ")\n",
    "\n",
    "# Set plot layout\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis=dict(visible=False),  # Hide x-axis\n",
    "        yaxis=dict(visible=False),  # Hide y-axis\n",
    "        zaxis=dict(visible=False),  # Hide z-axis\n",
    "        bgcolor=\"white\",  # Set the scene background to white\n",
    "    ),\n",
    "    paper_bgcolor=\"white\",  # Set the entire figure's background to white\n",
    "    plot_bgcolor=\"white\",  # Set the plotting area background to white\n",
    ")\n",
    "fig[\"layout\"][\"scene\"][\"aspectmode\"] = \"data\"\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
